{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b844c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents: Let's break down the differences between these Gemini models. The key distinctions often lie in their **capabilities, performance (speed and efficiency), and intended use cases**.\n",
      "\n",
      "Here's a brief breakdown:\n",
      "\n",
      "**Gemini 2.5 Pro:**\n",
      "\n",
      "*   **Capabilities:** The **most powerful and feature-rich** model in the 2.5 series. It excels at complex reasoning, understanding nuanced language, and handling a wide range of tasks including summarization, translation, question answering, creative writing, and more.\n",
      "*   **Performance:** While powerful, it can be **less performant (slower and more resource-intensive)** compared to its Flash counterparts due to its advanced capabilities.\n",
      "*   **Context Window:** Known for its **very large context window**, allowing it to process and understand massive amounts of information at once.\n",
      "*   **Intended Use:** Ideal for applications requiring **deep understanding, complex problem-solving, and high-quality output**, where precision and nuance are paramount. Think detailed analysis, creative content generation, and advanced coding assistance.\n",
      "\n",
      "**Gemini 2.5 Flash:**\n",
      "\n",
      "*   **Capabilities:** A **highly efficient and fast** model. It's optimized for speed and lower latency while still offering strong performance across many common tasks. It's designed to be a good balance between capability and efficiency.\n",
      "*   **Performance:** Significantly **faster and more efficient** than Gemini 2.5 Pro. This makes it suitable for real-time applications and scenarios where quick responses are crucial.\n",
      "*   **Context Window:** Likely inherits a substantial context window from the 2.5 architecture, but might have slightly less emphasis on extreme lengths compared to Pro for the sake of speed.\n",
      "*   **Intended Use:** Excellent for **dialogue-based applications, customer service chatbots, content summarization, and other use cases where speed and responsiveness are key**, without sacrificing too much quality.\n",
      "\n",
      "**Gemini 2.5 Flash-Lite:**\n",
      "\n",
      "*   **Capabilities:** This is an **even more streamlined and lightweight version** of Gemini 2.5 Flash. It's designed for maximum efficiency and minimal resource usage.\n",
      "*   **Performance:** The **fastest and most resource-efficient** model among the 2.5 series. It prioritizes speed and minimal computational overhead.\n",
      "*   **Context Window:** May have a **more limited context window** compared to 2.5 Flash and Pro to achieve its lightweight nature.\n",
      "*   **Intended Use:** Best for **on-device applications, edge computing, simpler chatbot interactions, and scenarios with very strict latency or resource constraints** where a highly capable but less resource-intensive model is needed.\n",
      "\n",
      "---\n",
      "\n",
      "**Gemini 2.0 Flash and 2.0 Flash-Lite:**\n",
      "\n",
      "These models belong to the previous generation (2.0) and represent earlier iterations of the Flash concept.\n",
      "\n",
      "**Gemini 2.0 Flash:**\n",
      "\n",
      "*   **Capabilities:** An **earlier version optimized for speed and efficiency** within the 2.0 generation. It offered a good balance of speed and performance for common NLP tasks.\n",
      "*   **Performance:** Faster and more efficient than the 2.0 Pro model (which isn't explicitly listed here but is implied as the more capable predecessor).\n",
      "*   **Context Window:** Likely had a **smaller context window** compared to the 2.5 series models.\n",
      "*   **Intended Use:** Suitable for applications that needed quick responses and were not as demanding in terms of deep reasoning or very long context.\n",
      "\n",
      "**Gemini 2.0 Flash-Lite:**\n",
      "\n",
      "*   **Capabilities:** The **most lightweight and efficient** model from the 2.0 generation, built for maximum speed and minimal resource footprint.\n",
      "*   **Performance:** The **fastest and most resource-efficient** of the 2.0 series.\n",
      "*   **Context Window:** Had a **more limited context window** than 2.0 Flash.\n",
      "*   **Intended Use:** Similar to 2.5 Flash-Lite, it was for on-device or resource-constrained environments, but with the overall capabilities of the older 2.0 architecture.\n",
      "\n",
      "**Key Takeaway:**\n",
      "\n",
      "The **\"2.5\" series represents a significant architectural advancement** over the \"2.0\" series, offering larger context windows and generally improved capabilities. Within each series, the naming convention generally indicates:\n",
      "\n",
      "*   **Pro:** Most capable, powerful, but potentially slower.\n",
      "*   **Flash:** Balanced between capability and speed/efficiency.\n",
      "*   **Flash-Lite:** Prioritizes speed and efficiency above all else, with potentially reduced capabilities and context window.\n",
      "\n",
      "When choosing, consider the trade-off between **power/accuracy and speed/efficiency** for your specific application needs.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "gemini_base_url = os.getenv(\"GEMINI_BASE_URL\")\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "gemini_model = \"gemini-2.5-flash-lite\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=gemini_base_url,\n",
    "    api_key=gemini_api_key\n",
    ")\n",
    "\n",
    "message = [{\"role\": \"user\", \"content\": \"Please give me a brief breakdown of the differences between Gemini Models: 2.5 Pro, 2.5 Flash, 2.5 Flash-Lite, 2.0 Flash, and 2.0 Flash-Lite. Thank you\"}]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=gemini_model,\n",
    "    messages=message\n",
    ")\n",
    "\n",
    "contents = response.choices[0].message.content\n",
    "\n",
    "print(f\"Contents: {contents}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
