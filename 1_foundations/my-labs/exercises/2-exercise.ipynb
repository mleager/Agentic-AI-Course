{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c166d7",
   "metadata": {},
   "source": [
    "# Chosen Workflow Pattern: Orchestrator - Worker\n",
    "\n",
    "1. Generate a complex question to ask all models\n",
    "2. Receive Models' Answers\n",
    "3. Store Model Name and Answer in a List of LLMResult Objects\n",
    "4. Have a reasoning model review all answers\n",
    "5. Return a List of Model Names with Ranking and brief Explanation of why they were given their rank\n",
    "\n",
    "__Potential JSON Responses__\n",
    "```json\n",
    "{\n",
    "    \"result\": {\n",
    "        \"o4-mini\": {\n",
    "            \"rank\": \"1\",\n",
    "            \"reason\": \"This model showed a deep understanding of...\"\n",
    "        },\n",
    "        \"gemma3:12b\": {\n",
    "            \"rank\": \"2\",\n",
    "            \"reason\": \"This model had a good understanding of...\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68d27a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58dee62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')\n",
    "GEMINI_BASE_URL = os.getenv('GEMINI_BASE_URL')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL')\n",
    "OLLAMA_API_KEY = \"ollama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a7c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Please respond only with the question, no explanation.\n"
     ]
    }
   ],
   "source": [
    "# Generate Question\n",
    "prompt = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "prompt += \"Please respond only with the question, no explanation.\"\n",
    "\n",
    "def generator(prompt: str) -> str:\n",
    "    message: list = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    openai = OpenAI()\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=message\n",
    "    )\n",
    "    return response.choices[0].message.content or \"\"\n",
    "\n",
    "question = generator(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dcc695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structures\n",
    "\n",
    "class LLMResult(BaseModel):\n",
    "    model: str\n",
    "    answer: str\n",
    "\n",
    "results: list[LLMResult] = []\n",
    "models = []\n",
    "ranks = []\n",
    "messages: list = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8062e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Models\n",
    "# Append Model Name and Answer to Results List\n",
    "\n",
    "def gemini_answer() -> None:\n",
    "    gemini = OpenAI(\n",
    "        api_key=GEMINI_API_KEY,\n",
    "        base_url=GEMINI_BASE_URL\n",
    "    )\n",
    "\n",
    "    model = \"gemini-2.5-flash-lite\"\n",
    "    try:\n",
    "        response = gemini.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        answer = response.choices[0].message.content or \"\"\n",
    "        results.append(LLMResult(model=model, answer=answer))\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "def openai_answer() -> None:\n",
    "    openai = OpenAI()\n",
    "\n",
    "    model = \"gpt-4o-mini\"\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        answer = response.choices[0].message.content or \"\"\n",
    "        results.append(LLMResult(model=model, answer=answer))\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "def ollama_answer() -> None:\n",
    "    ollama = OpenAI(\n",
    "        api_key=OLLAMA_API_KEY,\n",
    "        base_url=OLLAMA_BASE_URL\n",
    "    )\n",
    "\n",
    "    model = \"gemma3:12b\"\n",
    "    try:\n",
    "        response = ollama.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        answer = response.choices[0].message.content or \"\"\n",
    "        results.append(LLMResult(model=model, answer=answer))\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "openai_answer()\n",
    "gemini_answer()\n",
    "ollama_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a3542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Answer Together\n",
    "\n",
    "together = \"\"\n",
    "for res in results:\n",
    "    model = res.model\n",
    "    answer = res.answer\n",
    "    # models.append(model)\n",
    "    # answers.append(answer)\n",
    "    together += f\"## Response from {model}:\\n\"\n",
    "    together += f\"{answer}\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7471f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {   # as a Dict of Dicts\n",
    "#     \"result\": {\n",
    "#         \"<MODEL_NAME>\": {\n",
    "#             \"rank\": \"<RANK>\",\n",
    "#             \"why\": \"<BREIF EXPLANATION>\"\n",
    "#         },\n",
    "#         \"<MODEL_NAME>\": {\n",
    "#             \"rank\": \"<RANK>\",\n",
    "#             \"why\": \"<BREIF EXPLANATION>\"\n",
    "#         },\n",
    "#         ...\n",
    "#     }\n",
    "# }\n",
    "# {    # as a List of Dicts\n",
    "#     \"result\": [\n",
    "#         \"<MODEL_NAME>\": {\n",
    "#             \"rank\": \"1\",\n",
    "#             \"why\": \"<BREIF EXPLANATION>\"\n",
    "#         },\n",
    "#         \"<MODEL_NAME>\": {\n",
    "#             \"rank\": \"2\",\n",
    "#             \"why\": \"<BREIF EXPLANATION>\"\n",
    "#         },\n",
    "#         ...\n",
    "#     ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a0df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Prompt\n",
    "\n",
    "compare = f\"\"\"You are juding the responses of {len(results)} models. Each model was given the following question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Please rank each model between 1 and {len(results)}, with 1 being the best, and the lowest being the worst.\n",
    "The evaluation will be based off of: intelligence, reasoning, and response quality.\n",
    "Please output your response in JSON and only in JSON. It should contain the rank, and a brief explaination why, following this format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"result\": {\n",
    "        \"<MODEL_NAME>\": {\n",
    "            \"rank\": \"<RANK>\",\n",
    "            \"why\": \"<BREIF EXPLANATION>\"\n",
    "        },\n",
    "        \"<MODEL_NAME>\": {\n",
    "            \"rank\": \"<RANK>\",\n",
    "            \"why\": \"<BREIF EXPLANATION>\"\n",
    "        },\n",
    "        \"<MODEL_NAME>\": {\n",
    "            \"rank\": \"<RANK>\",\n",
    "            \"why\": \"<BREIF EXPLANATION>\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Thank you, and here is the Content to review for ranking:\n",
    "\n",
    "{together}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850e33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Score Response\n",
    "ranking_response = generator(compare)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81f19dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Results\n",
    "\n",
    "# 'ranking_response' is a String of JSON\n",
    "# it needs to be converted to a Python Dict\n",
    "\n",
    "# Convert from a String to JSON\n",
    "try:\n",
    "    json_ranking: dict = json.loads(ranking_response)\n",
    "    rankings: dict = json_ranking[\"result\"]\n",
    "    \n",
    "    # Sort by rank for better display\n",
    "    sorted_rankings = sorted(rankings.items(), key=lambda x: int(x[1][\"rank\"]))\n",
    "    \n",
    "    output = \"\"\n",
    "    for model_name, ranking_data in sorted_rankings:\n",
    "        rank = ranking_data[\"rank\"]\n",
    "        reason = ranking_data[\"reason\"]\n",
    "        output += f\"**Rank {rank}: {model_name}**\\n\"\n",
    "        output += f\"*Reason:* {reason}\\n\\n\"\n",
    "    \n",
    "    display(Markdown(output))\n",
    "    \n",
    "    # Also display the original question and summary\n",
    "    summary_output = f\"\"\"\n",
    "## Original Question\n",
    "{question}\n",
    "\n",
    "## Summary\n",
    "- **Total Models Evaluated:** {len(results)}\n",
    "- **Winner:** {sorted_rankings[0][0]} \n",
    "- **Evaluation Criteria:** Intelligence, reasoning, and response quality\n",
    "\"\"\"\n",
    "    display(Markdown(summary_output))\n",
    "    \n",
    "except (json.JSONDecodeError, KeyError) as e:\n",
    "    print(f\"Error parsing ranking response: {e}\")\n",
    "    print(f\"Raw response: {ranking_response}\")\n",
    "    \n",
    "    # Fallback display\n",
    "    output = \"**Error in ranking - showing raw response:**\\n\\n\"\n",
    "    output += ranking_response\n",
    "    display(Markdown(output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
