import os
from typing import List, Dict
from dotenv import load_dotenv
from openai import OpenAI
from IPython.display import Markdown, display

class Model:
    def __init__(self, name: str, api_key: str, base_url: str, model: str):
        self.name = name
        self.model = model
        self.messages: list = []
        self.client = OpenAI(base_url=base_url, api_key=api_key)

    def make_request(self, prompt: str) -> str:
        self.messages.append({"role": "user", "content": prompt})

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=self.messages,
            )

            answer = response.choices[0].message.content
            self.messages.append({"role": "assistant", "content": answer})
            return answer or ""

        except Exception as e:
            return f"Error making request: {str(e)}"

    def clear_messages(self) -> None:
        self.messages.clear()


def main():

    load_dotenv(override=True)

    gemini = Model("gemini", os.getenv("GEMINI_API_KEY"), os.getenv("GEMINI_BASE_URL"), "gemini-2.5-flash")  # type: ignore
    ollama = Model("ollama", os.getenv("OLLAMA_API_KEY"), os.getenv("OLLAMA_BASE_URL"), "gemma3:12b")        # type: ignore

    prompt = "Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. "
    prompt += "Please respond only with the question, no explanation."

    # Generate the question using Gemini
    question = gemini.make_request(prompt)
    print(f"Question generated by Gemini: {question}\n\n")
    gemini.clear_messages()

    ollama_answer = ollama.make_request(question)
    print(f"Ollama answer: {ollama_answer}\n\n")

    gemini_answer = gemini.make_request(question)
    print(f"Gemini answer: {gemini_answer}\n\n")

    answers = {
        "ollama": ollama_answer,
        "gemini": gemini_answer
    }

    comparison_prompt = f"Please choose which model has the best answer to the question:\n {question}\n\n"
    for model, answer in answers.items():
        comparison_prompt += f"# {model}\nanswer: {answer}\n\n"
    comparison_prompt += "Which is better and why?"

    result = gemini.make_request(comparison_prompt)
    display(Markdown(result))

if __name__ == "__main__":
    main()

